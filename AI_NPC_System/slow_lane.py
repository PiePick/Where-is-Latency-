# slow_lane.py
import config
from openai import OpenAI
import google.generativeai as genai
import asyncio

async def generate_response(user_input, fast_reaction=None):
    """
    Slow Lane: 'ì¡°ìš©íˆ' Ollama ì‹œë„ -> ì‹¤íŒ¨ ì‹œ 'ì¡°ìš©íˆ' Gemini ì „í™˜ -> ì„±ê³µí•œ ëª¨ë¸ë§Œ ë¡œê·¸ ì¶œë ¥
    """
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = (
        "You are a helpful and friendly NPC. "
        "Keep your response concise (within 2-3 sentences). "
        "Speak naturally like a human."
    )
    if fast_reaction:
        system_prompt += f" You already reacted with '{fast_reaction}'. Continue naturally."

    # ==========================================
    # ğŸ¥‡ ì‹œë„ 1: ë¡œì»¬ Ollama (Silent Try)
    # ==========================================
    try:
        # íƒ€ì„ì•„ì›ƒ 1.0ì´ˆ ì„¤ì •: ì•ˆ ì¼œì ¸ ìˆìœ¼ë©´ 1ì´ˆ ë§Œì— ë°”ë¡œ í¬ê¸°í•˜ê³  ë„˜ì–´ê°
        client = OpenAI(
            base_url=config.OLLAMA_URL,
            api_key='ollama',
            timeout=1.0 
        )

        response = client.chat.completions.create(
            model=config.OLLAMA_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_input}
            ],
            temperature=0.7,
        )
        
        # â˜… ì„±ê³µ ì‹œì—ë§Œ ë¡œê·¸ ì¶œë ¥
        print(f"ğŸ¢ [Slow Lane] âœ… Used Model: Local Ollama ({config.OLLAMA_MODEL})")
        return response.choices[0].message.content

    except Exception:
        # ì‹¤íŒ¨í•˜ë©´ ì•„ë¬´ ë§ë„ ì•ˆ í•˜ê³ (pass) ë°”ë¡œ ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°
        pass

    # ==========================================
    # ğŸ¥ˆ ì‹œë„ 2: í´ë¼ìš°ë“œ Gemini (Silent Try)
    # ==========================================
    try:
        if not config.GEMINI_API_KEY:
            return "âŒ Error: No Models Available."

        genai.configure(api_key=config.GEMINI_API_KEY)
        model = genai.GenerativeModel(config.GEMINI_MODEL)
        
        full_prompt = f"{system_prompt}\n\nUser Input: {user_input}"
        
        # ë¹„ë™ê¸° í˜¸ì¶œ
        response = await model.generate_content_async(full_prompt)
        
        # â˜… ì„±ê³µ ì‹œì—ë§Œ ë¡œê·¸ ì¶œë ¥
        print(f"ğŸ¢ [Slow Lane] âœ… Used Model: Cloud Gemini ({config.GEMINI_MODEL})")
        return response.text.strip()

    except Exception as e:
        print(f"âŒ [Fatal Error] All models failed: {e}")
        return "..."# slow_lane.py
import config
from openai import OpenAI
import google.generativeai as genai
import asyncio

async def generate_response(user_input, fast_reaction=None):
    """
    Slow Lane: 'ì¡°ìš©íˆ' Ollama ì‹œë„ -> ì‹¤íŒ¨ ì‹œ 'ì¡°ìš©íˆ' Gemini ì „í™˜ -> ì„±ê³µí•œ ëª¨ë¸ë§Œ ë¡œê·¸ ì¶œë ¥
    """
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = (
        "You are a helpful and friendly NPC. "
        "Keep your response concise (within 2-3 sentences). "
        "Speak naturally like a human."
    )
    if fast_reaction:
        system_prompt += f" You already reacted with '{fast_reaction}'. Continue naturally."

    # ==========================================
    # ğŸ¥‡ ì‹œë„ 1: ë¡œì»¬ Ollama (Silent Try)
    # ==========================================
    try:
        # íƒ€ì„ì•„ì›ƒ 1.0ì´ˆ ì„¤ì •: ì•ˆ ì¼œì ¸ ìˆìœ¼ë©´ 1ì´ˆ ë§Œì— ë°”ë¡œ í¬ê¸°í•˜ê³  ë„˜ì–´ê°
        client = OpenAI(
            base_url=config.OLLAMA_URL,
            api_key='ollama',
            timeout=1.0 
        )

        response = client.chat.completions.create(
            model=config.OLLAMA_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_input}
            ],
            temperature=0.7,
        )
        
        # â˜… ì„±ê³µ ì‹œì—ë§Œ ë¡œê·¸ ì¶œë ¥
        print(f"ğŸ¢ [Slow Lane] âœ… Used Model: Local Ollama ({config.OLLAMA_MODEL})")
        return response.choices[0].message.content

    except Exception:
        # ì‹¤íŒ¨í•˜ë©´ ì•„ë¬´ ë§ë„ ì•ˆ í•˜ê³ (pass) ë°”ë¡œ ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°
        pass

    # ==========================================
    # ğŸ¥ˆ ì‹œë„ 2: í´ë¼ìš°ë“œ Gemini (Silent Try)
    # ==========================================
    try:
        if not config.GEMINI_API_KEY:
            return "âŒ Error: No Models Available."

        genai.configure(api_key=config.GEMINI_API_KEY)
        model = genai.GenerativeModel(config.GEMINI_MODEL)
        
        full_prompt = f"{system_prompt}\n\nUser Input: {user_input}"
        
        # ë¹„ë™ê¸° í˜¸ì¶œ
        response = await model.generate_content_async(full_prompt)
        
        # â˜… ì„±ê³µ ì‹œì—ë§Œ ë¡œê·¸ ì¶œë ¥
        print(f"ğŸ¢ [Slow Lane] âœ… Used Model: Cloud Gemini ({config.GEMINI_MODEL})")
        return response.text.strip()

    except Exception as e:
        print(f"âŒ [Fatal Error] All models failed: {e}")
        return "..."